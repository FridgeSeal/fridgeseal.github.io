+++
title = " Better Operators - Or, sharing state but it's fun"
date = 2023-11-11
updated = 2023-11-12
[taxonomies]
  tags = ["distributed-systems"]
[extra]
  toc = true
+++

## Background

 I've recently been writing a Kubernetes Operator for work, and the experience has been _incredibly_ fun, and weirdly inspiring. It's like a complete inversion of the "normal" way we build (web/client-server) applications, and I think there's some _really, really_ powerful software-architectural ideas lurking in there which are probably being ignored because of the stigma of being attached to K8s, or because "they're just some weird thingy K8s does".

 ## [Underlying theory](#underlying-theory)

 The "normal" architecture and design principles for web services is something along the lines of:
 - REST/HTTP API, GRPC if you're lucky
 - Create / Read / Update / Delete API's as standard
  - Some designs favour CQRS which exhibits "read-write separation".
 - The database is your _private_ store of data.
  - DB's will be OLTP - as actions will be reading and mutating specific rows.
 - The API you expose is the "contract" with consuming applications.
Architecturally, it's all very "imperative", we perform actions and drive through state machines by holding some data and progressively mutating it. Compare this with a "message passing" or "event driven" style architecture, which is more "functional" in it's design.
Rather than holding state and mutating it according to data we send/receive, we receive some data, try and perform _our_ actions on it, and then pass it along to your consumers, or some kind of event stream (a la Kafka). In an imperative/OOP-style architecture, you make a request to some service, await a response and perform mutations to your object-of-responsibility accordingly. The equivalent in event-driven/message-passing land is passing it off as a message to the relevant consumer, and awaiting a message on _your_ incoming channels indicating that message `id: x` finished {successfully, errored}. The best designers will mix-and-match the best of these direct/message-passing API's in their software according to the ideal semantics of their desired system.

There's a _lot_ written about event-driven systems in software design. I think the first 15% of it is generated by Confluent trying to get people to use Kafka, and the rest is garbage-quality-Medium-blogspam, which really makes finding anything but the most introductory content difficult.

Most content about event-driven systems - at least within the spaces I move in - are about `edge triggered` systems, but there's another variant `level-triggered` systems. The former is what you are probably most familiar with if you've encountered event-driven systems before: you do work when you _receive_ the message. This is in contrast with `level-triggered` event system, in which _you do work while something is in a particular state_. I understand the latter is common in electrical engineering.

Kubernetes (henceforth K8s) uses a level-triggered system - controllers are invoked on the resources they manage, while those resources are in non-stable/non-terminal states[^1], and this is where the particularly interesting parts come in.

In K8s, you define what you want to run in the cluster, by writing up some horridly-verbose [yaml](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/), which gets quickly validated, before being written into `etcd` - K8s persistent, distributed KV-Store. When this write happens, the deployment controller is "woken up" and it'll read the `Deployment` yaml configs, attempt to make everything in there a reality, and then go back to sleep, this is known as the `reconciliation loop`. There's some important differences with your normal CRUD app here:
- if a reconciliation loop fails, a controller _does not_ attempt to rollback and cleanup.
  - instead, we simply mark it as failed, go back to sleep and try again later. This is because we might have "failed" for a few reason, and trying again might fix these. Ordering issues, pushing a deployment before the container was available, not fitting onto any existing machine and needing to wait for a new instance to come up are all valid examples of this.
- This radically simplifies a number of things:
  - we are explicitly expected to be able to deal with broken and degraded resources.
  - we don't have issues with failed rollouts/rollbacks causing invalid data or violating invariants.
  - It automatically gives us a nice way to handle ordering conflicts: simply roll everything out declaratively, and then anything that requires the existence of some other resource to be ready, will simply converge once all the conditions are met.



### [Footnotes](#footnotes)
[1.](^1) This is _really_ glossing over some stuff about how K8s works, but this is at least the design intent.